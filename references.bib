
@misc{wu_agentic_2025,
	title = {Agentic Reasoning: Reasoning {LLMs} with Tools for the Deep Research},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2502.04644},
	doi = {10.48550/ARXIV.2502.04644},
	shorttitle = {Agentic Reasoning},
	abstract = {We introduce Agentic Reasoning, a framework that enhances large language model ({LLM}) reasoning by integrating external tool-using agents. Unlike conventional {LLM}-based reasoning approaches, which rely solely on internal inference, Agentic Reasoning dynamically engages web search, code execution, and structured reasoning-context memory to solve complex problems requiring deep research and multi-step logical deduction. Our framework introduces the Mind Map agent, which constructs a structured knowledge graph to track logical relationships, improving deductive reasoning. Additionally, the integration of web-search and coding agents enables real-time retrieval and computational analysis, enhancing reasoning accuracy and decision-making. Evaluations on {PhD}-level scientific reasoning ({GPQA}) and domain-specific deep research tasks demonstrate that our approach significantly outperforms existing models, including leading retrieval-augmented generation ({RAG}) systems and closed-source {LLMs}. Moreover, our results indicate that agentic reasoning improves expert-level knowledge synthesis, test-time scalability, and structured problem-solving. The code is at: https://github.com/theworldofagents/Agentic-Reasoning.},
	publisher = {{arXiv}},
	author = {Wu, Junde and Zhu, Jiayuan and Liu, Yuyuan},
	urldate = {2025-04-14},
	date = {2025},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
}

@article{liu_harnessing_2025,
	title = {Harnessing {AI} for understanding scientific literature: Innovations and applications of chat-agent system in battery recycling research},
	volume = {49},
	issn = {24686069},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2468606925000267},
	doi = {10.1016/j.mtener.2025.101818},
	shorttitle = {Harnessing {AI} for understanding scientific literature},
	pages = {101818},
	journaltitle = {Materials Today Energy},
	shortjournal = {Materials Today Energy},
	author = {Liu, Rongfan and Zou, Zhi and Chen, Sihui and Liu, Yang and Wan, Jiayu},
	urldate = {2025-04-14},
	date = {2025-04},
	langid = {english},
}

@article{seth_artificial_2025,
	title = {Artificial intelligence versus human researcher performance for systematic literature searches: a study focusing on the surgical management of base of thumb arthritis},
	issn = {2349-6150},
	url = {https://www.oaepublish.com/articles/2347-9264.2024.99},
	doi = {10.20517/2347-9264.2024.99},
	shorttitle = {Artificial intelligence versus human researcher performance for systematic literature searches},
	abstract = {Aim:  In the digital age, artificial intelligence ({AI}) platforms have gradually replaced traditional manual techniques for information retrieval. However, their effectiveness in conducting academic literature searches remains unclear, necessitating a comparative assessment. This study examined the efficacy of {AI} search engines (Elicit, Consensus, {ChatGPT}) vs.  manual search for literature retrieval, focusing on the surgical management of trapeziometacarpal osteoarthritis.
            Methods:  The study was executed per the Cochrane Handbook for Systematic Reviews and {PRISMA} guidelines. {AI} platforms were given relevant keywords and prompts, while manual searches used {PubMed}, Cochrane {CENTRAL}, Web of Science, and Scopus databases from January 1901 to April 2024. The study focused on English-language randomized controlled trials ({RCTs}) comparing surgical management of trapeziometacarpal osteoarthritis ({TMCJ} {OA}). Two independent evaluators screened and extracted data from the studies. Primary outcomes involved the quality and relevancy of studies chosen by both search methods, evaluated by false positive rates and number of studies, including outcomes of interest.
            Results:  The manual search yielded the most results (6,018), followed by Elicit (4,980), Consensus (3,436), and {ChatGPT} (6). Elicit identified the highest number of {RCTs} (205) but also had the greatest false positive rate (94\%). Ultimately, the manual search identified 23 suitable studies, Elicit found 10, Consensus found 9, and {ChatGPT} identified only 1. No additional studies were found by {AI} search engines that were not discovered in the manual search.
            Conclusion:  The findings highlight the potential advantages and drawbacks of {AI} search engines for literature searches. While Elicit was prone to error, Consensus and {ChatGPT} were less comprehensive. Significant enhancements in the precision and thoroughness of {AI} search engines are required before they can be effectively utilized in academia.},
	journaltitle = {Plastic and Aesthetic Research},
	shortjournal = {Plast Aesthet Res},
	author = {Seth, Ishith and Lim, Bryan and Xie, Yi and Ross, Richard J. and Cuomo, Roberto and Rozen, Warren M.},
	urldate = {2025-04-14},
	date = {2025-01-06},
	file = {Volltext:C\:\\Users\\ramon\\Zotero\\storage\\VG4C6B8J\\Seth et al. - 2025 - Artificial intelligence versus human researcher pe.pdf:application/pdf},
}

@article{seth2025,
	title = {Artificial intelligence versus human researcher performance for systematic literature searches: a study focusing on the surgical management of base of thumb arthritis},
	author = {Seth, Ishith and Lim, Bryan and Xie, Yi and Ross, Richard J. and Cuomo, Roberto and Rozen, Warren M.},
	year = {2025},
	month = {01},
	date = {2025-01-06},
	journal = {Plastic and Aesthetic Research},
	doi = {10.20517/2347-9264.2024.99},
	url = {https://www.oaepublish.com/articles/2347-9264.2024.99}
}

@article{khalifa2024,
	title = {Using artificial intelligence in academic writing and research: An essential productivity tool},
	author = {Khalifa, Mohamed and Albadawy, Mona},
	year = {2024},
	date = {2024},
	journal = {Computer Methods and Programs in Biomedicine Update},
	pages = {100145},
	volume = {5},
	doi = {10.1016/j.cmpbup.2024.100145},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666990024000120},
	langid = {en}
}

@article{gusenbauer2025,
	title = {How to search for literature in systematic reviews and meta-analyses: A comprehensive step-by-step guide},
	author = {Gusenbauer, Michael and Gauster, Sebastian P.},
	year = {2025},
	month = {03},
	date = {2025-03},
	journal = {Technological Forecasting and Social Change},
	pages = {123833},
	volume = {212},
	doi = {10.1016/j.techfore.2024.123833},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0040162524006310},
	langid = {en}
}
